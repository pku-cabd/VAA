{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from esim.data import NLIDataset\n",
    "from esim.model import ESIM\n",
    "from esim.utils import correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data='/home/rongz/ESIM/data/preprocessed/quora/test_data.pkl'\n",
    "checkpoint='/home/rongz/ESIM/data/checkpoints/quora/best.pth.tar'\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_data, \"rb\") as pkl:\n",
    "    test_data = NLIDataset(pickle.load(pkl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['50018', '126924', '391187', '301889', '202497', '75122', '374364', '84209', '179700', '48882', '195953', '306946', '324676', '246367', '228987', '242869', '112473', '227323', '35973', '320170', '180350', '325484', '84643', '238134', '53437', '338378', '101440', '54980', '234913', '10882', '326051', '211707'], 'premise': tensor([[  2,   6,  43,  ...,   0,   0,   0],\n",
      "        [  2,  76, 206,  ...,   0,   0,   0],\n",
      "        [  2,  11,  20,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  2,  11, 162,  ...,   0,   0,   0],\n",
      "        [  2,   6,   8,  ...,   0,   0,   0],\n",
      "        [  2,   6,   8,  ...,   0,   0,   0]]), 'premise_length': tensor([12, 12,  9,  9, 11, 13, 18,  8, 11, 11, 16,  9, 14, 23,  9, 14, 10,  8,\n",
      "        13, 14, 11, 25, 10, 10, 12, 11, 14, 15,  9, 11, 13, 17]), 'hypothesis': tensor([[   2,   11,   14,  ...,    0,    0,    0],\n",
      "        [   2,   76,  206,  ...,    0,    0,    0],\n",
      "        [   2,   11,   14,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2, 6813,  782,  ...,    0,    0,    0],\n",
      "        [   2,    6,    8,  ...,    0,    0,    0],\n",
      "        [   2,   11,   71,  ...,    0,    0,    0]]), 'hypothesis_length': tensor([12, 10, 10,  8, 12, 14, 13,  9,  8, 23, 12, 10, 16, 32, 15, 19, 10, 12,\n",
      "        16, 17, 30, 23, 11, 13, 12, 10, 11,  8,  8, 30, 13, 14]), 'label': tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0])}\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(test_file, pretrained_file, batch_size=32):\n",
    "    \"\"\"\n",
    "    Test the ESIM model with pretrained weights on some dataset.\n",
    "\n",
    "    Args:\n",
    "        test_file: The path to a file containing preprocessed NLI data.\n",
    "        pretrained_file: The path to a checkpoint produced by the\n",
    "            'train_model' script.\n",
    "        vocab_size: The number of words in the vocabulary of the model\n",
    "            being tested.\n",
    "        embedding_dim: The size of the embeddings in the model.\n",
    "        hidden_size: The size of the hidden layers in the model. Must match\n",
    "            the size used during training. Defaults to 300.\n",
    "        num_classes: The number of classes in the output of the model. Must\n",
    "            match the value used during training. Defaults to 3.\n",
    "        batch_size: The size of the batches used for testing. Defaults to 32.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(20 * \"=\", \" Preparing for testing \", 20 * \"=\")\n",
    "\n",
    "    checkpoint = torch.load(pretrained_file)\n",
    "\n",
    "    # Retrieving model parameters from checkpoint.\n",
    "    vocab_size = checkpoint[\"model\"][\"_word_embedding.weight\"].size(0)\n",
    "    embedding_dim = checkpoint[\"model\"]['_word_embedding.weight'].size(1)\n",
    "    hidden_size = checkpoint[\"model\"][\"_projection.0.weight\"].size(0)\n",
    "    num_classes = checkpoint[\"model\"][\"_classification.4.weight\"].size(0)\n",
    "\n",
    "    print(\"\\t* Loading test data...\")\n",
    "    with open(test_file, \"rb\") as pkl:\n",
    "        test_data = NLIDataset(pickle.load(pkl))\n",
    "\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    print(\"\\t* Building model...\")\n",
    "    model = ESIM(vocab_size,\n",
    "                 embedding_dim,\n",
    "                 hidden_size,\n",
    "                 num_classes=num_classes,\n",
    "                 device=device).to(device)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    print(20 * \"=\",\n",
    "          \" Testing ESIM model on device: {} \".format(device),\n",
    "          20 * \"=\")\n",
    "    batch_time, total_time, accuracy, predict_df = test(model, test_loader)\n",
    "\n",
    "    print(\"-> Average batch processing time: {:.4f}s, total test time:\\\n",
    " {:.4f}s, accuracy: {:.4f}%\".format(batch_time, total_time, (accuracy*100)))\n",
    "    return predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    \"\"\"\n",
    "    Test the accuracy of a model on some labelled test dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The torch module on which testing must be performed.\n",
    "        dataloader: A DataLoader object to iterate over some dataset.\n",
    "\n",
    "    Returns:\n",
    "        batch_time: The average time to predict the classes of a batch.\n",
    "        total_time: The total time to process the whole dataset.\n",
    "        accuracy: The accuracy of the model on the input data.\n",
    "    \"\"\"\n",
    "    # Switch the model to eval mode.\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    time_start = time.time()\n",
    "    batch_time = 0.0\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    all_ids=[]\n",
    "    all_labels=[]\n",
    "    all_out_classes=[]\n",
    "\n",
    "    # Deactivate autograd for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch_start = time.time()\n",
    "\n",
    "            # Move input and output data to the GPU if one is used.\n",
    "            ids=batch[\"id\"]\n",
    "            premises = batch[\"premise\"].to(device)\n",
    "            premises_lengths = batch[\"premise_length\"].to(device)\n",
    "            hypotheses = batch[\"hypothesis\"].to(device)\n",
    "            hypotheses_lengths = batch[\"hypothesis_length\"].to(device)\n",
    "            labels = batch[\"label\"]\n",
    "            all_labels.extend(labels.tolist())\n",
    "            labels=labels.to(device)\n",
    "\n",
    "            _, probs = model(premises,\n",
    "                             premises_lengths,\n",
    "                             hypotheses,\n",
    "                             hypotheses_lengths)\n",
    "            _, out_classes = probs.max(dim=1)\n",
    "            all_out_classes.extend(out_classes.tolist())\n",
    "\n",
    "            accuracy += correct_predictions(probs, labels)\n",
    "            batch_time += time.time() - batch_start\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    predict_df=pd.DataFrame({'id':all_ids,'predict':all_out_classes})\n",
    "    \n",
    "    batch_time /= len(dataloader)\n",
    "    total_time = time.time() - time_start\n",
    "    accuracy /= (len(dataloader.dataset))\n",
    "    print(metrics.accuracy_score(all_labels,all_out_classes))\n",
    "    print(metrics.precision_score(all_labels,all_out_classes))\n",
    "    print(metrics.recall_score(all_labels,all_out_classes))\n",
    "    print(metrics.f1_score(all_labels,all_out_classes))\n",
    "\n",
    "    return batch_time, total_time, accuracy, predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================  Preparing for testing  ====================\n",
      "\t* Loading test data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NLIDataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4900b89a4a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-093fd4849ef7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(test_file, pretrained_file, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t* Loading test data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NLIDataset"
     ]
    }
   ],
   "source": [
    "predict_df=main(test_data,checkpoint,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_test_data='/home/rongz/ESIM/data/dataset/quora/test.tsv'\n",
    "origin_test_df=pd.read_csv(origin_test_data,header=None,delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_test_df[4]=predict_df['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the colume order\n",
    "origin_test_df=origin_test_df[[3,1,2,0,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_predict_path='/home/rongz/ESIM/data/predict/quora/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_test_df.to_csv(origin_predict_path, header=['id','sent1','sent2','label','predict'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_df=origin_test_df[(origin_test_df[0]==1) & (origin_test_df[4]==1)]\n",
    "true_negative_df=origin_test_df[(origin_test_df[0]==0) & (origin_test_df[4]==0)]\n",
    "false_positive_df=origin_test_df[(origin_test_df[0]==0) & (origin_test_df[4]==1)]\n",
    "false_negative_df=origin_test_df[(origin_test_df[0]==1) & (origin_test_df[4]==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_path='/home/rongz/ESIM/data/predict/quora/test_tp.csv'\n",
    "true_negative_path='/home/rongz/ESIM/data/predict/quora/test_tn.csv'\n",
    "false_positive_path='/home/rongz/ESIM/data/predict/quora/test_fp.csv'\n",
    "false_negative_path='/home/rongz/ESIM/data/predict/quora/test_fn.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_df.to_csv(true_positive_path, header=['id','sent1','sent2','label','predict'],index=False)\n",
    "true_negative_df.to_csv(true_negative_path, header=['id','sent1','sent2','label','predict'],index=False)\n",
    "false_positive_df.to_csv(false_positive_path, header=['id','sent1','sent2','label','predict'],index=False)\n",
    "false_negative_df.to_csv(false_negative_path, header=['id','sent1','sent2','label','predict'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
